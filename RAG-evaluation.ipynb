{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Retrieval Augmented Generation modification and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: httpx verion 0.27.0  is necessary to use the httpx.AsyncClient with groq. langchain issue that needs fixing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os  ### provides functionality to interact with the operating system. Use for file paths, e.g. the .env file\n",
    "import json \n",
    "import random \n",
    "from typing import Dict\n",
    "from pathlib import Path  # For working with file paths\n",
    "\n",
    "# Utility libraries\n",
    "from dotenv import load_dotenv  # For loading environment variables from a .env file\n",
    "import glob  # For matching file paths using patterns\n",
    "import tqdm  # For displaying progress bars in loops\n",
    "import pandas as pd  # For handling tabular data\n",
    "from datasets import Dataset  # For managing datasets (Hugging Face)\n",
    "\n",
    "# PDF handling\n",
    "from PyPDF2 import PdfReader  # For extracting text from PDF files\n",
    "\n",
    "# LangChain core functionality\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # For splitting text into manageable chunks\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate  # For defining and managing prompt templates\n",
    "from langchain.vectorstores import Chroma, FAISS  ### NOTE: changed from Chroma since its not supported # For creating vector stores for retrieval\n",
    "from langchain.embeddings import HuggingFaceEmbeddings  # For generating embeddings for text chunks\n",
    "\n",
    "# LangChain advanced components\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain  # For combining retrieved documents\n",
    "from langchain_core.output_parsers import StrOutputParser  # For parsing string outputs from models\n",
    "\n",
    "# Third-party AI model interfaces\n",
    "from langchain_openai import ChatOpenAI  # For using OpenAI models with LangChain\n",
    "from langchain_groq import ChatGroq  # For using Groq models with LangChain\n",
    "import openai  # For using OpenAI's API\n",
    "\n",
    "# For displaying notebook progress bars\n",
    "import tqdm\n",
    "import tqdm.notebook as notebook_tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bugfix for Chroma with SQLite\n",
    "\n",
    "This cell addresses a known issue with Chroma's dependency on `sqlite3`, which can conflict with certain Python environments. \n",
    "\n",
    "#### What this does:\n",
    "1. **Replaces `sqlite3` with `pysqlite3`:**\n",
    "   - Ensures compatibility by importing `pysqlite3` as a substitute for `sqlite3`.\n",
    "   - Updates the `sys.modules` mapping to ensure all imports of `sqlite3` use `pysqlite3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd()\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "DATABASES = {\n",
    "    'default': {\n",
    "        'ENGINE': 'django.db.backends.sqlite3',\n",
    "        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Environment Variables\n",
    "\n",
    "This cell loads sensitive environment variables such as API keys from a `.env` file. Using environment variables helps keep credentials secure and out of the source code.\n",
    "\n",
    "#### What this does:\n",
    "1. **`load_dotenv()`:**\n",
    "   - Loads environment variables from a `.env` file in the current working directory.\n",
    "   - A `.env` file typically contains key-value pairs (e.g., `GROQ_API_KEY=your_groq_api_key`).\n",
    "\n",
    "2. **Retrieve API Keys:**\n",
    "   - `os.getenv(\"GROQ_API_KEY\")`: Retrieves the GROQ API key.\n",
    "   - `os.getenv(\"OPENAI_API_KEY\")`: Retrieves the OpenAI API key.\n",
    "\n",
    "\n",
    "#### Notes:\n",
    "- Ensure you have a `.env` file in the root of your project directory with the required keys, for example:\n",
    "  ```plaintext\n",
    "  GROQ_API_KEY=your_groq_api_key\n",
    "  OPENAI_API_KEY=your_openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()   ### there can be only one .env file in the root directory\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")    ### will be provided I guess...?\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") ### either use own or provided key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Load and Extract Text from PDFs\n",
    "\n",
    "In this task, you will load multiple PDF files from a specified directory, read their content, and extract text. This text will later be used for processing and retrieval.\n",
    "\n",
    "#### Instructions:\n",
    "1. **Define the File Path:**\n",
    "\n",
    "2. **Iterate Over PDFs:**\n",
    "   - Use the `glob` library to find all files matching the `*.pdf` pattern in the specified directory.\n",
    "   - For each file, open it in binary mode (`\"rb\"`) using a `with` statement.\n",
    "\n",
    "3. **Extract Text:**\n",
    "   - Use the `PdfReader` library to read the PDF content.\n",
    "   - Iterate through the pages and extract text from each page.\n",
    "\n",
    "4. **Combine Text:**\n",
    "   - Concatenate the text from all pages into a single string (`text`).\n",
    "\n",
    "5. **Preview the Output:**\n",
    "   - Print the first 50 characters of the extracted text to verify that the content is loaded correctly.\n",
    "\n",
    "#### What to Do:\n",
    "- Run the cell and inspect the first 50 characters of the extracted text to confirm it works as expected.\n",
    "- If necessary, adjust the `glob_path` to point to the correct directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hyper tension in adul ts: \\ndiagnosis and manag eme'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load the pdf from the path\n",
    "glob_path = \"data/*.pdf\"    ### wildcard-search for all pdf files in the 'data' folder, works like in excel :-)\n",
    "text = \"\"\n",
    "for pdf_path in tqdm.tqdm(glob.glob(glob_path)):\n",
    "    with open(pdf_path, \"rb\") as file:  ### binary read mode, dont forget!\n",
    "        reader = PdfReader(file)\n",
    "         # Extract text from all pages in the PDF ### empty and 'NULL' pages are skipped \n",
    "        text += \" \".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "\n",
    "### own code? shows 50 characters from the 'text' string\n",
    "text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Split Extracted Text into Manageable Chunks\n",
    "\n",
    "#### Instructions:\n",
    "1. **Create a Text Splitter:**\n",
    "   - Use the `RecursiveCharacterTextSplitter` to split the text.\n",
    "   - Specify two key parameters:\n",
    "     - **`chunk_size` (2000):** The maximum number of characters in each chunk.\n",
    "     - **`chunk_overlap` (200):** The number of overlapping characters between consecutive chunks to maintain context continuity\n",
    "2. **Inspect the Chunks:**\n",
    "   - After splitting, verify the output by inspecting the `chunks` variable. Each chunk should be approximately 2000 characters long, with overlaps of 200 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ###NOTE: a good ration would be 5:1 up to 10:1 for the chunk_size and chunk_overlap\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "# Split the extracted text into manageable chunks\n",
    "chunks = splitter.split_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n",
      "Hyper tension in adul ts: \n",
      "diagnosis and manag emen t \n",
      "NICE guideline \n",
      "Published: 28 August 2019 \n",
      "Last updat ed: 21 No vember 2023 \n",
      "www .nice.or g.uk/guidance/ng136 \n",
      "© NICE 202 4. All right s reserved. Subject t o Notice of right s (https://www .nice.or g.uk/t erms-and-\n",
      "conditions#notice-of -right s). Your r esponsi bility \n",
      "The r ecommendations in t his guideline r epresent t he view of NICE, arriv ed at aft er car eful \n",
      "consideration of t he evidence a vailable. When e xercising t heir judgement, pr ofessionals \n",
      "and practitioners ar e expect ed to tak e this guideline fully int o account, alongside t he \n",
      "individual needs, pr eferences and v alues of t heir patient s or t he people using t heir ser vice. \n",
      "It is not mandat ory to apply t he recommendations, and t he guideline does not o verride t he \n",
      "responsibility t o mak e decisions appr opriat e to the cir cumstances of t he individual, in \n",
      "consultation wit h them and t heir f amilies and car ers or guar dian. \n",
      "All pr oblems (adv erse e vents) related to a medicine or medical de vice used f or treatment \n",
      "or in a pr ocedur e should be r epor ted to the Medicines and Healt hcare product s Regulat ory \n",
      "Agency using t he Yellow Car d Scheme . \n",
      "Local commissioners and pr oviders of healt hcare have a responsibility t o enable t he \n",
      "guideline t o be applied when individual pr ofessionals and people using ser vices wish t o \n",
      "use it. The y should do so in t he cont ext of local and national priorities f or funding and \n",
      "developing ser vices, and in light of t heir duties t o have due r egar d to the need t o eliminat e \n",
      "unlawful discrimination, t o adv ance equality of oppor tunity and t o reduce healt h \n",
      "inequalities. Not hing in t his guideline should be int erpreted in a wa y that w ould be \n",
      "inconsist ent wit h complying wit h those duties. \n",
      "Commissioners and pr oviders ha ve a responsibility t o promot e an en vironmentally \n",
      "sustainable healt h and car e syst em and should assess and r educe t he en vironmental\n"
     ]
    }
   ],
   "source": [
    "print(len(chunks)) ### number of chunks \n",
    "print(chunks[0]) ### first chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Create Embeddings for Text Chunks\n",
    "\n",
    "In this step, you will initialize the embedding model that will convert the text chunks into numerical representations (embeddings). These embeddings are essential for enabling similarity-based retrieval in the RAG system.\n",
    "\n",
    "#### Instructions:\n",
    "1. **Select an Embedding Model:**\n",
    "   - Use the `HuggingFaceEmbeddings` class to specify the embedding model.\n",
    "   - The model name provided here is `\"sentence-transformers/all-mpnet-base-v2\"`, a widely used embedding model for generating high-quality text representations.\n",
    "\n",
    "2. **Initialize the Model:**\n",
    "   - Pass the model name as an argument to `HuggingFaceEmbeddings` and assign the resulting object to the variable `embeddings`.\n",
    "\n",
    "\n",
    "#### What to Do:\n",
    "- Use the `HuggingFaceEmbeddings` class to load the specified embedding model.\n",
    "- Assign the loaded model to the `embeddings` variable.\n",
    "\n",
    "#### Documentation\n",
    "https://python.langchain.com/api_reference/huggingface/embeddings/langchain_huggingface.embeddings.huggingface.HuggingFaceEmbeddings.html#huggingfaceembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Create a Vector Store for Text Retrieval\n",
    "\n",
    "\n",
    "#### Instructions:\n",
    "1. **Generate the Vector Store:**\n",
    "   - Use the `Chroma.from_texts` method to create a vector store.\n",
    "   - Pass the `chunks` (text chunks) and the `embeddings` object (created in the previous step) as arguments.\n",
    "\n",
    "\n",
    "3. **Inspect the Output:**\n",
    "   - Optionally, inspect the `vector_store` to confirm that it is ready for retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/codespace/.local/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from faiss-cpu) (24.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "###NOTE --> new step, necessary to use FAISS. The '!' makes sure that the command is executed in the CLI ###\n",
    "!pip install faiss-cpu ### important, use the CPU version, not the GPU one\n",
    "\n",
    "vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "\n",
    "### NOTE: optionally, save the vector store to disk for later use\n",
    "### vector_store.save(\"vector_store.faiss\"). <-- dont forget the path!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Create a Retriever for the Vector Store\n",
    "\n",
    "In this step, you will create a retriever to query the vector store and fetch the most relevant text chunks for a given input query. The retriever uses the vector embeddings to perform similarity-based searches.\n",
    "\n",
    "#### Instructions:\n",
    "1. **Create the Retriever:**\n",
    "   - Use the `as_retriever` method on the `vector_store` to create a retriever.\n",
    "   - Set the `search_type` parameter to `\"mmr\"` (Maximal Marginal Relevance) to ensure diverse and relevant retrieval.\n",
    "   - Pass additional search settings using `search_kwargs`, such as:\n",
    "     - **`k`:** The number of chunks to retrieve (e.g., `k=3`).\n",
    "\n",
    "2. **Assign the Retriever:**\n",
    "   - Store the retriever in the variable `retriever` for later use in querying the vector store.\n",
    "\n",
    "3. **Verify the Retriever:**\n",
    "   - Ensure the retriever is correctly initialized and ready to handle queries.\n",
    "\n",
    "\n",
    "https://python.langchain.com/docs/integrations/vectorstores/chroma/#query-by-turning-into-retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})\n",
    "    ### k=3 means that the top 3 most similar chunks are returned for each query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='medical r ecords, alongside t he coded diagnostic entr y. [NICE 2017 , amended \\nBTS/NICE/SIGN 202 4] Asthma: diagnosis, monit oring and chr onic ast hma management (BTS, NICE, SIGN)\\n(NG2 45)\\n© NICE 202 4. All right s reserved. Subject t o Notice of right s (https://www .nice.or g.uk/t erms-and-\\nconditions#notice-of -right s).Page 9 of\\n64 Physical examina tion \\n1.1.4 Examine people wit h suspect ed ast hma t o identify e xpirat ory polyphonic wheez e \\nand signs of ot her causes of r espirat ory sympt oms but be awar e that e ven if \\nexamination r esult s are normal, t he person ma y still ha ve ast hma. [NICE 2017] \\nInitial tr eatmen t and obje ctive tests f or acu te sym ptoms a t \\npresen tation \\n1.1.5 Treat people immediat ely if t hey are acut ely unw ell or highly sympt omatic at \\npresentation, and per form objectiv e tests that ma y help suppor t a diagnosis of \\nasthma (f or example, eosinophil count , fractional e xhaled nitric o xide [F eNO], \\nspirometr y or peak e xpirat ory flow [PEF] bef ore and aft er br onchodilat or) if t he \\nequipment is a vailable. [NICE 2017 , amended BTS/NICE/SIGN 202 4] \\n1.1.6 If objectiv e tests for ast hma cannot be done immediat ely for people who ar e \\nacutely unw ell or highly sympt omatic at pr esentation, carr y them out when acut e \\nsympt oms ha ve been contr olled, and advise people t o contact t heir healt hcare \\nprofessional immediat ely if t hey become unw ell while waiting t o have objectiv e \\ntests. [NICE 2017 , amended BTS/NICE/SIGN 202 4] \\n1.1.7 Be awar e that t he result s of spir ometr y and FeNO t ests may be aff ected in people \\nwho ha ve been tr eated wit h inhaled cor ticost eroids (t he test result s are mor e \\nlikely to be normal). [NICE 2017] \\n1.2 O bjective tests f or diagnosing asthma in adul ts, \\nyoung pe ople and childr en ag ed 5 to 16 wi th a \\nhistor y sug gestive of asthma \\nAdults \\nSee also algorit hm A f or a summar y of objectiv e tests for diagnosing ast hma in adult s and'),\n",
       " Document(metadata={}, page_content='conditions such as f ood aller gies. \\nBronchial challeng e test \\nA test t o measur e airway responsiv eness (br onchial r esponsiv eness). It is per formed b y \\ngiving small incr ement s of a br onchoconstrict or (most commonly met hacholine) and \\nmeasuring t he FEV 1 after each dose until it f alls b y a pr edet ermined amount (usually 20% \\nfrom baseline). \\nBronchial h yperr esponsi veness \\nA measur e of ho w easily br onchospasm can be induced in t he air ways. It is measur ed \\nusing a br onchial challenge t est. \\nBronchodila tor r eversibility \\nA measur e of t he ability t o reverse obstruction in t he air ways using medicines t hat widen Asthma: diagnosis, monit oring and chr onic ast hma management (BTS, NICE, SIGN)\\n(NG2 45)\\n© NICE 202 4. All right s reserved. Subject t o Notice of right s (https://www .nice.or g.uk/t erms-and-\\nconditions#notice-of -right s).Page 33 of\\n64 the air ways (br onchodilat ors). \\nEosinop hil c ount \\nThe number of eosinophils (a type of whit e blood cell) measur ed in a blood sample. Their \\nlevels ar e raised in ast hma and ot her aller gic diseases, and less commonly wit h malignant \\ndiseases, parasit e infections, r eactions t o some medicines, and a small number of rar e \\ndiseases. \\nFeNO test \\nA test t hat measur es the amount of nitric o xide (NO) pr esent on e xhalation, usually \\nexpressed in par ts per billion. \\nFEV1 \\nThe amount of air t hat can be f orcibly e xhaled fr om t he lungs in one second (f orced \\nexpirat ory volume in one second). \\nLeuk otriene r eceptor an tagonist \\nA type of oral medicine t hat blocks cyst einyl leuk otrienes, used in t he tr eatment of ast hma \\nand seasonal aller gies. Also kno wn as leuk otriene modifiers. \\nLong-ac ting be ta2 agonist \\nA long-acting medicine t hat act s on beta-r ecept ors in t he air way to relax air way smoot h \\nmuscle and r elieve sympt oms of ast hma. \\nLong-ac ting m uscarinic r eceptor an tagonist'),\n",
       " Document(metadata={}, page_content=\"the person's pr edict ed FEV 1, and using t his paramet er a change of 10% or mor e is \\nabnormal. Using t he mor e traditional means of e xpressing t he change as a per centage of \\nthe baseline FEV 1, increased r eversibility w ould be 12% or mor e in adult s and childr en. In \\nadult s, the change should also be 200 ml or mor e. The committ ee agr eed t o include bot h \\nways of measuring r eversibility in it s recommendations. \\nAn optimal cut -off v alue is also difficult t o giv e for FeNO (fractional e xhaled nitric o xide). \\nTher e is good e vidence t hat F eNO le vels incr ease wit h age and wit h height, and ideally \\nnormal ranges w ould be a vailable which corr ect f or these f actors. Ho wever, there are \\ncurrently no standar d char ts and F eNO equipment does not giv e an age/height corr ected \\noutput. Alt hough not ideal, t he committ ee agr eed t hat t hey need t o suggest a simple cut -\\noff v alue. And because F eNO is t he first, and possibly t he only , test in t he recommended \\nsequences in bot h adult s and childr en they agr eed t hat t he value should be r easonably \\nhigh so t hat it w ould be specific, ackno wledging t hat t his sacrifices a degr ee of sensitivity . Asthma: diagnosis, monit oring and chr onic ast hma management (BTS, NICE, SIGN)\\n(NG2 45)\\n© NICE 202 4. All right s reserved. Subject t o Notice of right s (https://www .nice.or g.uk/t erms-and-\\nconditions#notice-of -right s).Page 43 of\\n64 Cut-offs of 50 ppb in adult s and 35 ppb in childr en w ere agr eed. \\nNo e vidence was a vailable f or diagnostic t ests in childr en under 5 . The age at which a \\nchild can co-operat e wit h tests will v ary, but t he committ ee agr eed t hat it is usually \\nnecessar y to manage t hese childr en pragmatically based on sympt oms and signs only . \\nAdult s \\nSeveral t ests sho wed good specificity f or ast hma, wit h values o ver 80% f or blood \\neosinophils, F eNO ( cut-off v alues 40-50 ppb ), peak e xpirat ory flow (PEF) v ariability ,\")]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.invoke(\"How do I diagnose Asthma?\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Build a RAG Model Function\n",
    "\n",
    "In this task, you will combine all the steps from the previous tasks to create a reusable function for building a Retrieval-Augmented Generation (RAG) model. The function will process raw text documents, generate embeddings, and store them in a vector store for efficient retrieval.\n",
    "\n",
    "#### What You Need to Do:\n",
    "\n",
    "1. **Define the Function:**\n",
    "   - Create a function named `build_rag_model` with the following parameters:\n",
    "     - **`texts` (List[str]):** A list of raw documents or text strings to process.\n",
    "     - **`embedding_model` (str):** The name of the Hugging Face embedding model to use.\n",
    "     - **`chunk_size` (int):** The maximum size of each text chunk.\n",
    "     - **`chunk_overlap` (int):** The overlap size between consecutive chunks.\n",
    "\n",
    "2. **Implement the Steps:**\n",
    "   - **Step 1:** **Split Text into Chunks**\n",
    "     - Use `RecursiveCharacterTextSplitter` to split the provided `texts` into chunks.\n",
    "     - Ensure the function handles all documents in the list and combines the resulting chunks.\n",
    "     - Print the number of generated chunks for debugging purposes.\n",
    "\n",
    "   - **Step 2:** **Generate Embeddings**\n",
    "     - Initialize a `HuggingFaceEmbeddings` object using the provided `embedding_model`.\n",
    "     - Use this object to generate embeddings for the text chunks.\n",
    "\n",
    "   - **Step 3:** **Create a Vector Store**\n",
    "     - Use `Chroma.from_texts` to create a vector store from the chunks and their embeddings.\n",
    "     - Print the number of chunks stored in the vector store for confirmation.\n",
    "\n",
    "3. **Return the Vector Store:**\n",
    "   - The function should return the vector store so it can be used for retrieval tasks.\n",
    "\n",
    "\n",
    "#### Example Usage:\n",
    "- Call the function like this:\n",
    "  ```python\n",
    "  retriever = build_rag_model(\n",
    "      texts=[\"Asthma is a chronic condition.\", \"Hypertension is persistently high blood pressure.\"],\n",
    "      embedding_model=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "      chunk_size=200,\n",
    "      chunk_overlap=50\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE: this function does not include the initial text-from-pdf extraction, but only the steps from the chunking to the retrieval\n",
    "\n",
    "def build_rag_model(texts, embedding_model, chunk_value):\n",
    "    \"\"\"\n",
    "    Build a RAG model (vector store) with the specified embedding model, chunk size, and overlap.\n",
    "\n",
    "    Args:\n",
    "        texts (List[str]): List of raw documents or text to process.\n",
    "        embedding_model (str): Name of the Hugging Face embedding model.\n",
    "        chunk_value (List): List containing chunk length and overlap\n",
    "\n",
    "    Returns:\n",
    "        VectorStore: A Chroma vector store with embeddings for retrieval.\n",
    "    \"\"\"\n",
    "    print(f\"Building RAG model with embedding model: {embedding_model}, chunk size: {chunk_value[0]}, overlap: {chunk_value[1]}\")\n",
    "    \n",
    "    # Step 1: Split texts into chunks\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_value[0], chunk_overlap=chunk_value[1])\n",
    "    chunks = splitter.split_text(text)\n",
    "    \n",
    "    print(f\"Generated {len(chunks)} chunks from {len(texts)} documents.\")\n",
    "\n",
    "    # Step 2: Generate embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "    \n",
    "    # Step 3: Create vector store\n",
    "    vector_store = FAISS.from_texts(chunks, embeddings)\n",
    "    \n",
    "    print(f\"Vector store created with {len(chunks)} chunks.\")\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Define a Generative Model for Question Answering\n",
    "\n",
    "In this task, you will define and initialize a generative model that can answer user questions based on a given context. This involves creating a prompt template, setting up an output parser, and initializing the language model for generation.\n",
    "\n",
    "#### What You Need to Do:\n",
    "\n",
    "1. **Define the Prompt Template:**\n",
    "   - Use the `PromptTemplate` class to define a template that specifies how user questions and the associated context are structured.\n",
    "   - Your template should:\n",
    "     - Include placeholders for the context (`{context}`) and question (`{question}`).\n",
    "     - Provide clear instructions for the model to generate answers based only on the context.\n",
    "\n",
    "2. **Initialize the Prompt Template:**\n",
    "   - Set the `template` argument to the system template:\n",
    "     ```python\n",
    "     system_template = \"\"\"\n",
    "     Answer the users question based on the below context:\n",
    "     <context> {context} </context>\n",
    "     Here is the question: <question> {question} </question>\n",
    "     \"\"\"\n",
    "     ```\n",
    "   - Specify the `input_variables` as `[\"context\", \"question\"]` to define the placeholders.\n",
    "\n",
    "3. **Set Up the Output Parser:**\n",
    "   - Use the `StrOutputParser` to parse the string output from the model.\n",
    "\n",
    "4. **Initialize the Generative Model:**\n",
    "   - Use the `ChatGroq` class to set up a generative model with the following parameters:\n",
    "     - **`model`:** Specify the model name (e.g., `\"llama-3.2-3b-preview\"`).\n",
    "     - **`temperature`:** Set to `0` for deterministic outputs.\n",
    "     - **`max_tokens`:** Set to `None` to allow the model to decide the output length.\n",
    "     - **`timeout`:** Set to handle timeouts during generation.\n",
    "     - **`max_retries`:** Define the number of retries in case of failure.\n",
    "\n",
    "\n",
    "Groq documentation: https://python.langchain.com/v0.1/docs/integrations/chat/groq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the template for answering user questions based on a provided context\n",
    "system_template = \"\"\"\n",
    "Answer the users question based on the below context:\n",
    "<context> {context} </context>\n",
    "Here is the question: <question> {question} </question>\n",
    "\"\"\"\n",
    "# Create a prompt template for the question-answering system\n",
    "question_answering_prompt = PromptTemplate(template=system_template, input_variables=[\"context\", \"question\"])\n",
    "output_parser = StrOutputParser()\n",
    "    ### where does 'output_parser' come from? --> from the 'langchain_core.output_parsers' import\n",
    "\n",
    "# Initialize the generative model for question answering\n",
    "model = ChatGroq(model=\"llama-3.2-3b-preview\", temperature=0, max_tokens=None, timeout=None, max_retries=2,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Build the RAG Chain for Question Answering\n",
    "\n",
    "In this task, you will create a **RAG (Retrieval-Augmented Generation) Chain** that connects the components you’ve defined so far: the prompt template, the generative model, and the output parser. This chain orchestrates the process of answering user questions by sequentially formatting inputs, generating answers, and parsing outputs.\n",
    "\n",
    "#### What You Need to Do:\n",
    "\n",
    "1. **Chain the Components:**\n",
    "   - Use the pipe operator (`|`) to sequentially combine the components:\n",
    "     - **`question_answering_prompt`:** Formats the user question and context into the structured template.\n",
    "     - **`model`:** The generative model processes the formatted input and generates a response.\n",
    "     - **`output_parser`:** Parses the raw response from the model into a structured and usable format.\n",
    "\n",
    "2. **Assign the Chain:**\n",
    "   - Store the combined components into the variable `rag_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ###NOTE: 'pipeline' operator used for sequential processing. At this point I'm not sure if the steps have a direct dependency from one to another\n",
    "rag_chain = question_answering_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test your chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, diagnosing asthma involves a combination of physical examination, objective tests, and medical history. Here's a step-by-step guide:\n",
      "\n",
      "1. **Physical examination**: Examine people with suspected asthma to identify expiratory polyphonic wheeze and signs of other respiratory symptoms. Be aware that even if examination results are normal, the person may still have asthma.\n",
      "\n",
      "2. **Initial treatment and objective tests**: Treat people immediately if they are acutely unwell or highly symptomatic at presentation. Perform objective tests that may help support a diagnosis of asthma, such as:\n",
      "   - Eosinophil count (the number of eosinophils in a blood sample)\n",
      "   - Fractional exhaled nitric oxide (FeNO) test\n",
      "   - Spirometry or peak expiratory flow (PEF) before and after bronchodilator\n",
      "\n",
      "3. **Bronchial challenge test**: A test to measure airway responsiveness (bronchial responsiveness). It involves giving small increments of a bronchoconstrictor (most commonly methacholine) and measuring the FEV1 after each dose until it falls by a predetermined amount (usually 20% from baseline).\n",
      "\n",
      "4. **Bronchodilator reversibility test**: A measure of the ability to reverse obstruction in the airways using medicines that widen the airways (bronchodilators).\n",
      "\n",
      "5. **Other tests**: Other tests may include a bronchial hypresponsiveness test, leukotriene receptor antagonist, long-acting beta2 agonist, and others.\n",
      "\n",
      "The National Institute for Health and Care Excellence (NICE) recommends the following cut-off values for FeNO and PEF:\n",
      "\n",
      "- FeNO: 50 ppb in adults and 35 ppb in children\n",
      "- PEF variability: >200 ml in adults and >12% in children\n",
      "\n",
      "It's essential to note that the diagnosis of asthma should be made based on a combination of clinical symptoms, medical history, and objective test results. The NICE guidelines provide a framework for diagnosing asthma, but the specific tests and cut-off values may vary depending on the individual case.\n"
     ]
    }
   ],
   "source": [
    "query = \"How do I diagnose Asthma?\"\n",
    "print(rag_chain.invoke({\"context\": docs, \"question\": query}))\n",
    "    ### input variables as a DICT format "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Define a Function to Answer Questions Using RAG\n",
    "\n",
    "In this task, you will create a function that leverages the RAG (Retrieval-Augmented Generation) system to answer user questions. The function will retrieve relevant documents from the knowledge index and use the RAG chain to generate a response.\n",
    "\n",
    "#### What You Need to Do:\n",
    "\n",
    "1. **Define the Function:**\n",
    "   - Name the function `answer_with_rag`.\n",
    "   - Specify the following arguments:\n",
    "     - **`question` (str):** The user's query.\n",
    "     - **`rag_chain`:** The RAG chain you built earlier for formatting, generating, and parsing responses.\n",
    "     - **`retriever` (VectorStore):** The vector store containing document embeddings for retrieval.\n",
    "\n",
    "2. **Implement the Steps:**\n",
    "   - **Step 1:** Retrieve Relevant Documents\n",
    "     - Use the `retriever` to retrieve documents related to the query.\n",
    "\n",
    "   - **Step 2:** Prepare the Input for the RAG Chain\n",
    "     - Create a dictionary named `rag_input` with the following keys:\n",
    "       - **`context`:** A list of retrieved document texts.\n",
    "       - **`question`:** The user query.\n",
    "\n",
    "   - **Step 3:** Generate an Answer\n",
    "     - Pass the `rag_input` to the `rag_chain` using the `invoke` method.\n",
    "     - Store the generated response in the variable `answer`.\n",
    "\n",
    "3. **Return the Results:**\n",
    "   - The function should return a tuple containing:\n",
    "     - **`answer` (str):** The generated response to the question.\n",
    "     - **`relevant_docs` (List[str]):** The list of retrieved document texts used for answering the query.\n",
    "\n",
    "4. **Test the Function:**\n",
    "   - Test the function with sample questions and ensure it retrieves relevant documents and generates accurate answers.\n",
    "\n",
    "#### Example Usage:\n",
    "- Call the function like this:\n",
    "  ```python\n",
    "  answer, relevant_docs = answer_with_rag(\n",
    "      question=\"What are the symptoms of asthma?\",\n",
    "      rag_chain=rag_chain,  # Your defined RAG chain\n",
    "      knowledge_index=knowledge_index  # Your vector store retriever\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_rag(\n",
    "    question,\n",
    "    rag_chain,  # Replace LLM with the rag_chain directly\n",
    "    retriever):\n",
    "    \"\"\"\n",
    "    Answer a question using RAG with the given knowledge index and rag_chain.\n",
    "\n",
    "    Args:\n",
    "        question (str): The user's question.\n",
    "        rag_chain: The RAG chain that takes context and question as input and generates the answer.\n",
    "        knowledge_index (VectorStore): The vector store used for retrieving relevant documents.\n",
    "        num_retrieved_docs (int): Number of documents to retrieve initially.\n",
    "        num_docs_final (int): Number of documents to include in the final context.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, List[str]]: A tuple containing the generated answer and the relevant documents used.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = retriever.invoke(question)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # Keep only the text of the documents\n",
    "        ### necessary step. The 'retriever' returns a list of 'Document' objects, but we need a list of pure strings as an input for the RAG chain\n",
    "\n",
    "\n",
    "    # Limit to the top N final documents\n",
    "    relevant_docs = relevant_docs   ###How is this supposed to work? It it necessary?\n",
    "\n",
    "    # Pass the documents and the question to the RAG chain\n",
    "    rag_input = {\n",
    "        \"context\": relevant_docs,  # Pass the documents as a list\n",
    "        \"question\": question,      # Pass the user query ###NOTE: dont forget the final comma, otherwise it will not be recognized as a tuple\n",
    "    }\n",
    "\n",
    "    # Use the RAG chain to generate an answer\n",
    "    answer = rag_chain.invoke(rag_input)\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Based on the provided context, asthma is a chronic respiratory condition that requires diagnosis, monitoring, and management. The context mentions that asthma can be differentiated from symptoms caused by recurrent viral infections in children under 5 years old. It also highlights the importance of diagnosing asthma through various tests, including bronchial challenge testing, spirometry, FeNO measurement, and skin prick testing.',\n",
       " ['BTS ISBN: 9 78-1-917 619-01-1 \\nNICE ISBN: 9 78-1-47 31-6612- 7 \\nSIGN ISBN: 9 78-1-909103-92-4 Asthma: diagnosis, monit oring and chr onic ast hma management (BTS, NICE, SIGN)\\n(NG2 45)\\n© NICE 202 4. All right s reserved. Subject t o Notice of right s (https://www .nice.or g.uk/t erms-and-\\nconditions#notice-of -right s).Page 64 of\\n64',\n",
       "  'conditions#notice-of -right s).Page 45 of\\n64 unlik ely and can be ruled out wit hout r esor ting t o bronchial challenge t esting. Alt hough \\ntaking blood f or IgE is in vasive, it does ha ve the adv antage t hat an eosinophil count could \\nalso be obtained, and if t his is abo ve 0.5 x 109 \\nper litr e, it w ould suppor t a diagnosis of \\nasthma. \\nThe committ ee w ere awar e that t here can be dela ys in obtaining spir ometr y, FeNO \\nmeasur ement s or skin prick t esting, and t hat it ma y not be possible t o get blood samples \\nfrom some childr en. It is hoped t hat access t o these t ests will impr ove. But if t he tests are \\nnot a vailable or t here is a significant dela y in obtaining t hem, t he committ ee agr eed it \\nwould be r easonable t o use PEF v ariability as a substitut e rule-in t est. \\nThe best single t est is a br onchial challenge t est, but t hese ar e also not r eadily a vailable \\nand cannot be done in primar y car e. If t here is still diagnostic doubt aft er per forming ot her \\ntests, the committ ee agr eed t hat a r eferral t o an ast hma specialist should be made f or a \\nsecond opinion, including consideration of a challenge t est. \\nFurther r esear ch \\nAlthough t here is e vidence underpinning each of t he tests included in t he recommended \\ndiagnostic sequences f or adult s and f or childr en aged 5 t o 16 y ears, t he committ ee \\nackno wledged t hat t he sequences t hemselv es ha ve not been t ested. The clinical and \\ncost-effectiv eness of t he recommended diagnostic pr ocess should be f ormally e valuat ed. \\nChildr en under 5 \\nThe main issue in t his age gr oup is diff erentiating ast hma fr om sympt oms caused b y \\nrecurr ent viral inf ections. The committ ee w ere awar e of e vidence out side t he review of \\ndiagnostic t ests sho wing t hat ast hma is mor e likely than r ecurr ent viral wheez e when t he \\nepisodes ar e frequent or se vere, when t hey occur in t he absence of ot her signs of viral',\n",
       "  \"requir ement. \\nThe committ ee concluded t hat F eNO monit oring was cost -effectiv e in adult s but ma y not \\nbe in childr en. It was not possible on t he curr ent e vidence t o say what t he optimum \\nfrequency of monit oring should be, but t he committ ee agr eed t hat an appr opriat e \\noppor tunity w ould be t o mak e a routine measur ement at t he person's r egular r eview \\n(which will be an annual r eview f or most people). \\nThe F eNO le vel is a pr oxy measur e of air way inflammation. It can t herefore be v ery useful \\nin det ermining ho w to adjust tr eatment, or as an indicat or of tr eatment adher ence, when a \\nperson wit h ast hma has poor sympt om contr ol. Con versely , when sympt om contr ol is \\nexcellent and t he possibility of r educing maint enance t herap y arises, a normal F eNO le vel \\nprovides helpful r eassurance. The committ ee therefore agr eed t hat a F eNO measur ement \\nshould be consider ed whene ver a change in maint enance t herap y might be appr opriat e. \\nHow the r ecommenda tions mig ht affect practice \\nAsthma contr ol questionnair es ar e already r ecommended as par t of an annual r eview . \\nTher efore, no change t o practice is anticipat ed. The r ecommendations on pulmonar y \\nfunction ar e expect ed to reduce t he use of PEF monit oring. \\nMeasur ement of F eNO is incr easingly used in secondar y car e ast hma clinics, but in \\nprimar y car e only a minority of GP practices ha ve on-sit e access t o the test. R egular F eNO \\nmonit oring r epresent s a significant change in practice because most people wit h ast hma \\nare managed in primar y car e. This change will also carr y a cost. The committ ee not ed that \\nFeNO measur ement is also useful in diagnosing ast hma (see section 1 .2 on objectiv e tests \\nfor diagnosing ast hma), and incr eased access t o the test will t herefore be of dual benefit. Asthma: diagnosis, monit oring and chr onic ast hma management (BTS, NICE, SIGN)\\n(NG2 45)\"])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_with_rag(\"what is asthma?\", rag_chain, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Set Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation: Function to Call the OpenAI API\n",
    "\n",
    "This function interacts with the OpenAI API to generate responses based on a given prompt. It provides a simple wrapper for querying the API and returning the generated output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call the OpenAI API\n",
    "def call_llm(prompt: str):\n",
    "    \"\"\"\n",
    "    Calls the OpenAI API to generate a response for a given prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt for the LLM.\n",
    "        model (str): The OpenAI model to use (default is \"gpt-4\").\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response from the LLM.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Prompt for QA Generation\n",
    "\n",
    "This prompt template defines the instructions for generating factoid-style question-answer (QA) pairs based on a given context. It is specifically crafted to create search-engine-style questions and concise, factual answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Question-Answer (QA) Pairs\n",
    "\n",
    "1. **Set the Number of QA Pairs to Generate:**\n",
    "   - **`N_GENERATIONS`:** Specifies the maximum number of QA pairs to generate. Here, it is set to `30`.\n",
    "\n",
    "2. **Sample Chunks:**\n",
    "   - Randomly selects `N_GENERATIONS` chunks from the `chunks` using `random.sample`.\n",
    "\n",
    "3. **Loop Over Chunks:**\n",
    "   - For each sampled chunk:\n",
    "     - **Step 1:** Format the prompt:\n",
    "       - Replaces the `{context}` placeholder in `QA_generation_prompt` with the text of the current chunk.\n",
    "     - **Step 2:** Call the LLM:\n",
    "       - Sends the formatted prompt to the `call_llm` function to generate a question and its corresponding answer.\n",
    "     - **Step 3:** Extract Question and Answer:\n",
    "       - Parses the output to extract the `Factoid question` and `Answer` fields.\n",
    "     - **Step 4:** Validate and Append:\n",
    "       - Ensures the answer is less than 300 characters long.\n",
    "       - Appends the valid `context`, `question`, and `answer` to the `outputs` list.\n",
    "\n",
    "4. **Handle Errors:**\n",
    "   - If an error occurs during QA generation (e.g., malformed output), it skips the current chunk and logs the error.\n",
    "\n",
    "5. **Display the Results:**\n",
    "   - After processing all chunks, prints the generated QA pairs for inspection.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This is Important:\n",
    "- This step generates a dataset of factoid-style QA pairs, which is essential for:\n",
    "  - Evaluating the RAG system's performance.\n",
    "  - Testing how well the QA pipeline retrieves relevant context and generates accurate answers.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example Output:\n",
    "- Each generated entry in `outputs` will look like this:\n",
    "  ```python\n",
    "  {\n",
    "      \"context\": \"Asthma is a chronic condition that affects the airways.\",\n",
    "      \"question\": \"What is asthma?\",\n",
    "      \"answer\": \"A chronic condition that affects the airways.\"\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 30 QA couples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 41929.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n",
      "Skipped a context due to error: name 'OpenAI' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###no idea what this block is supposed to do ###\n",
    "\n",
    "N_GENERATIONS = 30\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "# Generate QA pairs\n",
    "outputs = []\n",
    "for sampled_context in tqdm.tqdm(random.sample(chunks, min(N_GENERATIONS, len(chunks)))): ### min() goes for whatever is smaller, \n",
    "    # Generate QA couple\n",
    "    try:\n",
    "        formatted_prompt = QA_generation_prompt.format(context=sampled_context)\n",
    "        output_QA_couple = call_llm(formatted_prompt)\n",
    "        # Extract question and answer from the output\n",
    "        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0].strip()\n",
    "        answer = output_QA_couple.split(\"Answer: \")[-1].strip()\n",
    "        # Validate and append to outputs\n",
    "        assert len(answer) < 300, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped a context due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Print generated outputs\n",
    "for output in outputs:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(outputs).head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Filtering with Critiques\n",
    "\n",
    "These prompts are designed to evaluate the quality of the generated factoid questions based on specific criteria: **groundedness**, **relevance**, and **stand-alone clarity**. Each prompt asks the LLM to provide a score and a rationale for the rating.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Groundedness Critique Prompt**\n",
    "\n",
    "##### Purpose:\n",
    "- To evaluate how well the question can be answered using the provided context.\n",
    "- Ensures the question is clearly and unambiguously grounded in the given text.\n",
    "\n",
    "##### Details:\n",
    "- The rating scale is from **1 to 5**:\n",
    "  - **1:** The question cannot be answered at all using the context.\n",
    "  - **5:** The question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "#### **2. Relevance Critique Prompt**\n",
    "\n",
    "##### Purpose:\n",
    "- To assess how useful the question is for developers, particularly in machine learning or NLP applications.\n",
    "- Ensures the question is aligned with the needs of the target audience (e.g., developers building with Hugging Face).\n",
    "\n",
    "##### Details:\n",
    "- The rating scale is from **1 to 5**:\n",
    "  - **1:** The question is not useful at all.\n",
    "  - **5:** The question is highly useful and relevant to the audience.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Stand-Alone Critique Prompt**\n",
    "\n",
    "##### Purpose:\n",
    "- To determine if the question can be understood without additional context.\n",
    "- Ensures the question is self-contained and meaningful to someone with domain knowledge or access to related documentation.\n",
    "\n",
    "##### Details:\n",
    "- The rating scale is from **1 to 5**:\n",
    "  - **1:** The question depends on additional information (e.g., \"in the context\" or \"in the document\").\n",
    "  - **5:** The question is fully understandable and stand-alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critique QA Pairs Using LLM Prompts\n",
    "\n",
    "In this task, you will evaluate each generated QA pair using the previously defined critique prompts for **groundedness**, **relevance**, and **stand-alone clarity**. The goal is to score and document the quality of each question based on the provided context and criteria.\n",
    "\n",
    "---\n",
    "\n",
    "#### What This Code Does:\n",
    "\n",
    "1. **Iterate Over QA Outputs:**\n",
    "   - Loops through the `outputs` list, which contains the generated QA pairs (`context`, `question`, `answer`).\n",
    "\n",
    "2. **Generate Evaluations:**\n",
    "   - For each QA pair:\n",
    "     - **Groundedness:** Uses the `question_groundedness_critique_prompt` to evaluate if the question is answerable based on the given context.\n",
    "     - **Relevance:** Uses the `question_relevance_critique_prompt` to evaluate if the question is useful for the intended audience.\n",
    "     - **Stand-alone Clarity:** Uses the `question_standalone_critique_prompt` to evaluate if the question is understandable without additional context.\n",
    "\n",
    "3. **Call the LLM for Each Criterion:**\n",
    "   - Sends the formatted prompt for each criterion to the LLM using `call_llm`.\n",
    "   - Stores the response in the `evaluations` dictionary under the respective criterion.\n",
    "\n",
    "4. **Parse the Results:**\n",
    "   - Extracts the **`Total rating`** (score) and **`Evaluation`** (text rationale) from the LLM's response.\n",
    "   - Updates the `output` dictionary with the scores and evaluations for each criterion.\n",
    "\n",
    "5. **Handle Errors Gracefully:**\n",
    "   - If any part of the process fails (e.g., LLM output is malformed), the loop skips the current QA pair and continues with the next one.\n",
    "\n",
    "6. **Update Outputs:**\n",
    "   - Adds the critique scores and rationale to each QA pair in the `outputs` list.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example Output:\n",
    "\n",
    "Each `output` in the `outputs` list will be updated with fields like these:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"context\": \"Asthma is a chronic condition that affects the airways.\",\n",
    "    \"question\": \"What is asthma?\",\n",
    "    \"answer\": \"A chronic condition that affects the airways.\",\n",
    "    \"groundedness_score\": 5,\n",
    "    \"groundedness_eval\": \"The question is fully answerable based on the provided context.\",\n",
    "    \"relevance_score\": 4,\n",
    "    \"relevance_eval\": \"This question is relevant to an audience seeking general knowledge about asthma.\",\n",
    "    \"standalone_score\": 5,\n",
    "    \"standalone_eval\": \"The question is clear and understandable without additional context.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating critique for each QA couple...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "for output in tqdm.tqdm(outputs):\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n",
    "        ),\n",
    "        \"relevance\": call_llm(\n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "        \"standalone\": call_llm(\n",
    "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "    }\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            score, eval = (\n",
    "                int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
    "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
    "            )\n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval,\n",
    "                }\n",
    "            )\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation: Filtering and Preparing the Evaluation Dataset\n",
    "\n",
    "In this step, we transform the evaluated QA pairs into a structured dataset, filter them based on their scores, and prepare the final dataset for further evaluation or model training.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step-by-Step Breakdown:\n",
    "\n",
    "\n",
    "2. **Convert QA Pairs to a DataFrame:**\n",
    "   - `generated_questions = pd.DataFrame.from_dict(outputs)`:\n",
    "     - Converts the `outputs` list (which now includes QA pairs and their scores) into a pandas DataFrame for easier manipulation and analysis.\n",
    "\n",
    "3. **Display the Evaluation Dataset (Before Filtering):**\n",
    "   - Prints a subset of columns:\n",
    "     - **`question`:** The generated question.\n",
    "     - **`answer`:** The corresponding answer.\n",
    "     - **`groundedness_score`, `relevance_score`, `standalone_score`:** Scores assigned during the critique step.\n",
    "   - This provides an overview of the dataset before applying any filtering criteria.\n",
    "\n",
    "4. **Filter the QA Pairs:**\n",
    "   - Keeps only QA pairs that meet the following conditions:\n",
    "     - **`groundedness_score` >= 4:** The question is well-anchored in the provided context.\n",
    "     - **`standalone_score` >= 4:** The question is clear and understandable without additional context.\n",
    "   - **Note:** The `relevance_score` is not used for filtering here, but it remains part of the dataset for reference.\n",
    "\n",
    "5. **Display the Filtered Dataset:**\n",
    "   - Prints the filtered DataFrame to show the high-quality QA pairs that passed the criteria.\n",
    "\n",
    "6. **Convert to a Hugging Face Dataset:**\n",
    "   - `eval_dataset = datasets.Dataset.from_pandas(generated_questions, split=\"train\", preserve_index=False)`:\n",
    "     - Converts the filtered pandas DataFrame into a Hugging Face `Dataset` object, which is commonly used for training and evaluation in NLP tasks.\n",
    "     - The `split=\"train\"` argument designates this as a training split.\n",
    "     - `preserve_index=False` ensures the index from the pandas DataFrame is not carried over to the `Dataset`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Purpose of This Step:\n",
    "\n",
    "1. **Dataset Refinement:**\n",
    "   - Filters out low-quality QA pairs to ensure only well-scored questions and answers are included in the final dataset.\n",
    "   - Focuses on groundedness and stand-alone clarity to improve the overall utility and reliability of the dataset.\n",
    "\n",
    "2. **Final Dataset Preparation:**\n",
    "   - Converts the data into a format suitable for further evaluation or training machine learning models, such as Hugging Face models.\n",
    "\n",
    "3. **Quality Assurance:**\n",
    "   - Provides a visual overview of the dataset before and after filtering, allowing for manual inspection of the data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset before filtering:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['question', 'answer', 'groundedness_score', 'relevance_score',\\n       'standalone_score'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m generated_questions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(outputs)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation dataset before filtering:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m display(\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mgenerated_questions\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroundedness_score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelevance_score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstandalone_score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m generated_questions \u001b[38;5;241m=\u001b[39m generated_questions\u001b[38;5;241m.\u001b[39mloc[\n\u001b[1;32m     18\u001b[0m     (generated_questions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroundedness_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;241m&\u001b[39m (generated_questions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandalone_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     20\u001b[0m ]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m============================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['question', 'answer', 'groundedness_score', 'relevance_score',\\n       'standalone_score'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "generated_questions = generated_questions.loc[\n",
    "    (generated_questions[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions[\"standalone_score\"] >= 4)\n",
    "]\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(generated_questions, split=\"train\", preserve_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation: Running RAG Tests\n",
    "\n",
    "This function evaluates the performance of the RAG (Retrieval-Augmented Generation) system by comparing the system's generated answers to the true answers in a test dataset. The results are saved to a file for further analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### What This Function Does:\n",
    "\n",
    "1. **Prepare the Output File:**\n",
    "   - Attempts to load existing test results from `output_file`:\n",
    "     - If the file exists, appends new results to the previous ones.\n",
    "     - If the file does not exist, initializes an empty `outputs` list.\n",
    "\n",
    "2. **Iterate Over the Evaluation Dataset:**\n",
    "   - Loops through the `eval_dataset`, which contains the test questions, true answers, and source documents.\n",
    "\n",
    "3. **Skip Already Evaluated Questions:**\n",
    "   - Checks if a question has already been tested (i.e., exists in the loaded `outputs`).\n",
    "   - Skips the question if it has already been evaluated.\n",
    "\n",
    "4. **Run the RAG System:**\n",
    "   - Calls the `answer_with_rag` function to:\n",
    "     - Retrieve relevant documents using the `knowledge_index`.\n",
    "     - Generate an answer using the LLM (Language Model).\n",
    "\n",
    "5. **Print Results (Optional):**\n",
    "   - If `verbose=True`, prints the following details for manual inspection:\n",
    "     - The input question.\n",
    "     - The generated answer.\n",
    "     - The true answer from the dataset.\n",
    "\n",
    "6. **Save Results:**\n",
    "   - Constructs a dictionary containing:\n",
    "     - The question and its true answer.\n",
    "     - The source document.\n",
    "     - The generated answer.\n",
    "     - The retrieved documents used to generate the answer.\n",
    "     - Test settings, if provided.\n",
    "   - Appends the result to the `outputs` list and saves it to the `output_file` in JSON format.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Example Usage:\n",
    "\n",
    "```python\n",
    "run_rag_tests(\n",
    "    eval_dataset=eval_dataset,  # Test dataset\n",
    "    llm=rag_chain,  # RAG chain (includes retrieval and generation)\n",
    "    knowledge_index=knowledge_index,  # Vector store retriever\n",
    "    output_file=\"rag_test_results.json\",  # File to save the results\n",
    "    verbose=True,  # Print results for inspection\n",
    "    test_settings={\n",
    "        \"embedding_model\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        \"chunk_size\": 200,\n",
    "        \"overlap\": 50,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset,\n",
    "    llm,\n",
    "    knowledge_index,\n",
    "    output_file: str,\n",
    "    verbose = True,\n",
    "    test_settings = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation: Setting Up the Evaluation Prompt\n",
    "\n",
    "This step defines the evaluation prompt that will be used to assess the quality of responses generated by the RAG system. The prompt follows a structured format to ensure consistent and objective evaluation based on a predefined scoring rubric.\n",
    "\n",
    "---\n",
    "\n",
    "#### Purpose of the Evaluation Prompt:\n",
    "\n",
    "1. **Define the Evaluation Task:**\n",
    "   - The LLM is tasked with comparing a generated response (`response`) to a reference answer (`reference_answer`) and scoring its quality based on specific criteria.\n",
    "\n",
    "2. **Provide a Score Rubric:**\n",
    "   - A detailed rubric is included to guide the LLM in assigning scores. The rubric ensures that scoring is based strictly on correctness, accuracy, and factual alignment with the reference answer.\n",
    "\n",
    "3. **Standardize Output:**\n",
    "   - The LLM is instructed to:\n",
    "     - Write a detailed feedback summary addressing the evaluation criteria.\n",
    "     - Assign a numerical score between 1 and 5, strictly adhering to the rubric.\n",
    "     - Format the output using the required structure, including `[RESULT]`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Structure of the Prompt:\n",
    "\n",
    "1. **Task Description:**\n",
    "   - Specifies the evaluation task and output format.\n",
    "   - Emphasizes that the feedback must focus on the score rubric and avoid general evaluations.\n",
    "\n",
    "2. **Instruction to Evaluate:**\n",
    "   - The instruction or context that prompted the response.\n",
    "\n",
    "3. **Response to Evaluate:**\n",
    "   - The generated response being evaluated.\n",
    "\n",
    "4. **Reference Answer:**\n",
    "   - The ideal answer that would receive a perfect score of 5.\n",
    "\n",
    "5. **Score Rubrics:**\n",
    "   - Provides explicit criteria for scoring:\n",
    "     - **Score 1:** Completely incorrect and inaccurate.\n",
    "     - **Score 5:** Completely correct, accurate, and factual.\n",
    "\n",
    "6. **Feedback Section:**\n",
    "   - Guides the LLM to write structured feedback followed by the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation: Evaluating Generated Answers\n",
    "\n",
    "This function evaluates the quality of answers generated by the RAG system using a predefined evaluation prompt and a scoring language model. The evaluation process is iterative and updates the results file in place for checkpointing and saving progress.\n",
    "\n",
    "---\n",
    "\n",
    "#### What This Function Does:\n",
    "\n",
    "1. **Initialize the Evaluation Environment:**\n",
    "   - **`answer_path`:** Path to the JSON file containing the generated answers.\n",
    "   - **`eval_chat_model`:** The language model used for evaluation (e.g., GPT-4).\n",
    "   - **`evaluator_name`:** A string identifier for the evaluator (e.g., \"GPT4\").\n",
    "   - **`evaluation_prompt_template`:** The prompt template that defines how the evaluation task is framed.\n",
    "\n",
    "2. **Load Existing Results:**\n",
    "   - If the `answer_path` file exists, loads the previously saved results into `answers`.\n",
    "   - This ensures that previously evaluated answers are not re-evaluated, saving time and resources.\n",
    "\n",
    "3. **Iterate Over Generated Answers:**\n",
    "   - For each entry in `answers`:\n",
    "     - **Check for Prior Evaluation:** If the answer has already been evaluated by the specified evaluator (`eval_score_{evaluator_name}`), skip it.\n",
    "     - **Prepare the Evaluation Prompt:**\n",
    "       - Uses `evaluation_prompt_template` to format the instruction, response, and reference answer into the structured prompt.\n",
    "     - **Evaluate the Response:**\n",
    "       - Sends the prompt to the `eval_chat_model` (e.g., GPT-4) and receives the evaluation result.\n",
    "     - **Parse the Result:**\n",
    "       - Extracts `feedback` and `score` from the model's output, splitting on `[RESULT]` to ensure the expected format is followed.\n",
    "\n",
    "4. **Update the Results:**\n",
    "   - Adds the following fields to the current experiment:\n",
    "     - **`eval_score_{evaluator_name}`:** The numeric score assigned by the evaluator.\n",
    "     - **`eval_feedback_{evaluator_name}`:** The detailed feedback provided by the evaluator.\n",
    "   - Saves the updated `answers` list back to the `answer_path` file after each iteration for checkpointing.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### Example Usage:\n",
    "\n",
    "```python\n",
    "evaluate_answers(\n",
    "    answer_path=\"rag_test_results.json\",  # File containing generated answers\n",
    "    eval_chat_model=ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0),  # Evaluation model\n",
    "    evaluator_name=\"GPT4\",  # Identifier for the evaluator\n",
    "    evaluation_prompt_template=evaluation_prompt_template,  # Evaluation prompt template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "evaluator_name = \"GPT4\"\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm.tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation: Running the Complete RAG Evaluation Pipeline\n",
    "\n",
    "This script integrates all the steps covered so far to run a full evaluation of the RAG system across different configurations. It includes embedding creation, chunking, retrieval, generation, and evaluation in a loop to test multiple setups.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step-by-Step Breakdown:\n",
    "\n",
    "1. **Create Output Directory:**\n",
    "   - Ensures that a directory named `./output` exists to store the results.\n",
    "\n",
    "2. **Define Configurations:**\n",
    "   - **`embedding_models`:** List of embedding models to test (e.g., `\"sentence-transformers/all-mpnet-base-v2\"`).\n",
    "   - **`chunk_sizes`:** List of `(chunk_size, overlap)` tuples to test different chunking strategies.\n",
    "     - Example:\n",
    "       - `[2000, 100]`: Chunks of 2000 characters with 100-character overlap.\n",
    "       - `[5000, 500]`: Larger chunks of 5000 characters with 500-character overlap.\n",
    "\n",
    "3. **Iterate Over Configurations:**\n",
    "   - Loops through all combinations of `embedding_models` and `chunk_sizes`.\n",
    "   - Constructs a unique `settings_name` for each combination to name the output files clearly.\n",
    "\n",
    "4. **Build the Knowledge Base:**\n",
    "   - Calls the `build_rag_model` function with:\n",
    "     - `texts`: The input data (e.g., pre-split chunks of the documents).\n",
    "     - `embedding_model`: The current embedding model.\n",
    "     - `chunk_size` and `chunk_overlap`: Parameters for splitting the text.\n",
    "   - Converts the resulting vector store into a retriever (`knowledge_index`) for querying.\n",
    "\n",
    "5. **Run the RAG System:**\n",
    "   - Iterates through the `eval_dataset` (assumed to contain questions and true answers).\n",
    "   - Calls the `answer_with_rag` function to:\n",
    "     - Retrieve relevant documents using the `knowledge_index`.\n",
    "     - Generate answers using the RAG chain (`rag_chain`).\n",
    "   - Appends the results to a list, including:\n",
    "     - The question, true answer, generated answer, and retrieved documents.\n",
    "\n",
    "6. **Save Results:**\n",
    "   - Saves the answers to a JSON file named based on the configuration (`output_file_name`).\n",
    "\n",
    "7. **Evaluate the Answers:**\n",
    "   - Calls `evaluate_answers` to:\n",
    "     - Critique and score the generated answers using the LLM evaluator (`eval_chat_model`).\n",
    "     - Update the saved results with scores and feedback for each answer.\n",
    "\n",
    "---\n",
    "\n",
    "#### What This Script Accomplishes:\n",
    "\n",
    "1. **End-to-End Workflow:**\n",
    "   - Automates the entire RAG pipeline, from embedding creation to evaluation.\n",
    "\n",
    "2. **Flexible Testing:**\n",
    "   - Tests multiple configurations for embeddings and chunking, enabling comparative analysis.\n",
    "\n",
    "3. **Results Storage:**\n",
    "   - Saves intermediate and final results to disk for reproducibility and further analysis.\n",
    "\n",
    "4. **Scoring and Feedback:**\n",
    "   - Generates actionable feedback and numerical scores for the generated answers.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example Workflow:\n",
    "\n",
    "**Configuration 1:**\n",
    "- **Embedding Model:** `\"sentence-transformers/all-mpnet-base-v2\"`\n",
    "- **Chunk Size:** `2000`\n",
    "- **Overlap:** `100`\n",
    "\n",
    "**Sample Output File:**\n",
    "- `./output/rag_chunk:2000_embeddings:sentence-transformers~all-mpnet-base-v2.json`\n",
    "\n",
    "**Generated Results:**\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"question\": \"What are the symptoms of asthma?\",\n",
    "        \"generated_answer\": \"Asthma symptoms include shortness of breath and chest tightness.\",\n",
    "        \"true_answer\": \"Shortness of breath, wheezing, and chest tightness.\",\n",
    "        \"retrieved_docs\": [\"Document 1 text\", \"Document 2 text\"],\n",
    "        \"eval_score_GPT4\": \"4\",\n",
    "        \"eval_feedback_GPT4\": \"The response is mostly correct, but it omits 'wheezing' from the symptoms listed in the reference answer.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk:[2000, 100]_embeddings:sentence-transformers~all-mpnet-base-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Building RAG model with embedding model: sentence-transformers/all-mpnet-base-v2, chunk size: 2000, overlap: 100\n",
      "Generated 123 chunks from 130 documents.\n",
      "Vector store created with 123 chunks.\n",
      "Running RAG...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'eval_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning RAG...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m answers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[43meval_dataset\u001b[49m):  \u001b[38;5;66;03m# Assume eval_dataset is iterable\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     question \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     29\u001b[0m     true_answer \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "# Configurations\n",
    "embedding_models = [\"sentence-transformers/all-mpnet-base-v2\"]  # Add more models as needed\n",
    "chunk_sizes = [[2000,100], [5000,500]]  # Add more chunk sizes as needed\n",
    "\n",
    "# Iterate through configurations\n",
    "for chunk_size in chunk_sizes:\n",
    "    for embedding_model in embedding_models:\n",
    "        settings_name = f\"chunk:{chunk_size}_embeddings:{embedding_model.replace('/', '~')}\"\n",
    "        output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "        print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "        print(\"Loading knowledge base embeddings...\")\n",
    "        # Use rag_builder to create the vector store\n",
    "        vector_store = build_rag_model(\n",
    "            texts=chunks,  # Assuming `chunks` contains pre-split text data\n",
    "            embedding_model=embedding_model,\n",
    "            chunk_value=chunk_size\n",
    "        )\n",
    "        retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2})\n",
    "\n",
    "        print(\"Running RAG...\")\n",
    "        answers = []\n",
    "        for sample in tqdm.tqdm(eval_dataset):  # Assume eval_dataset is iterable\n",
    "            question = sample[\"question\"]\n",
    "            true_answer = sample[\"answer\"]\n",
    "\n",
    "            # Call the RAG function to get the generated answer\n",
    "            generated_answer, relevant_docs = answer_with_rag(\n",
    "                question=question,\n",
    "                rag_chain=rag_chain,  # Replace with your RAG chain\n",
    "                retriever=retriever,\n",
    "            )\n",
    "\n",
    "            answers.append({\n",
    "                \"question\": question,\n",
    "                \"generated_answer\": generated_answer,\n",
    "                \"true_answer\": true_answer,\n",
    "                \"relevant_docs\": relevant_docs,\n",
    "            })\n",
    "\n",
    "        # Save results to file\n",
    "        with open(output_file_name, \"w\") as f:\n",
    "            json.dump(answers, f)\n",
    "\n",
    "        print(\"Running evaluation...\")\n",
    "        evaluate_answers(\n",
    "            output_file_name,\n",
    "            eval_chat_model,\n",
    "            evaluator_name,\n",
    "            evaluation_prompt_template,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation: Aggregating and Normalizing Evaluation Results\n",
    "\n",
    "This code collects evaluation results from multiple JSON files, combines them into a single dataset, and normalizes the evaluation scores for further analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step-by-Step Breakdown:\n",
    "\n",
    "1. **Initialize an Empty List:**\n",
    "   - `outputs = []`: Prepares a list to store the results from all JSON files.\n",
    "\n",
    "2. **Load JSON Files:**\n",
    "   - `glob.glob(\"./output/*.json\")`: Finds all JSON files in the `./output` directory.\n",
    "   - For each file:\n",
    "     - Loads the JSON content into a pandas DataFrame using `pd.DataFrame`.\n",
    "     - Adds a new column, `settings`, to store the filename, indicating the configuration used for generating the results.\n",
    "     - Appends the DataFrame to the `outputs` list.\n",
    "\n",
    "3. **Combine All Results:**\n",
    "   - `pd.concat(outputs)`: Concatenates all DataFrames in the `outputs` list into a single DataFrame named `result`.\n",
    "\n",
    "4. **Normalize Evaluation Scores:**\n",
    "   - **Convert to Integer:**\n",
    "     - `result[\"eval_score_GPT4\"].apply(lambda x: int(x) if isinstance(x, str) else 1)`:\n",
    "       - Ensures all scores are integers, with a fallback value of `1` for non-numeric entries.\n",
    "   - **Normalize to Range [0, 1]:**\n",
    "     - `result[\"eval_score_GPT4\"] = (result[\"eval_score_GPT4\"] - 1) / 4`:\n",
    "       - Transforms the scores from the range `[1, 5]` to `[0, 1]`:\n",
    "         - Subtracts `1` to shift the range to `[0, 4]`.\n",
    "         - Divides by `4` to scale the range to `[0, 1]`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Purpose of This Step:\n",
    "\n",
    "1. **Aggregate Results:**\n",
    "   - Combines evaluation results from multiple configurations into a single dataset, making it easier to compare and analyze performance.\n",
    "\n",
    "2. **Normalize Scores:**\n",
    "   - Converts the raw scores into a standardized format (`[0, 1]`) for consistent interpretation and comparison across configurations.\n",
    "\n",
    "3. **Preserve Configuration Context:**\n",
    "   - Adds the `settings` column to retain information about which configuration each set of results corresponds to.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msettings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m file\n\u001b[1;32m      5\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[0;32m----> 6\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_score_GPT4\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_score_GPT4\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_score_GPT4\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_score_GPT4\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "\n",
    "outputs = []\n",
    "for file in glob.glob(\"./output/*.json\"):\n",
    "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "    output[\"settings\"] = file\n",
    "    outputs.append(output)\n",
    "result = pd.concat(outputs)\n",
    "result[\"eval_score_GPT4\"] = result[\"eval_score_GPT4\"].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "result[\"eval_score_GPT4\"] = (result[\"eval_score_GPT4\"] - 1) / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation: Calculating and Sorting Average Scores by Configuration\n",
    "\n",
    "This code calculates the average evaluation scores for each configuration and sorts them in ascending order to identify the best and worst-performing setups.\n",
    "\n",
    "---\n",
    "\n",
    "#### Purpose of This Step:\n",
    "\n",
    "1. **Performance Comparison:**\n",
    "   - Calculates the overall effectiveness of each configuration by averaging the normalized evaluation scores across all questions.\n",
    "   - Highlights configurations that consistently produce better results.\n",
    "\n",
    "2. **Identify Trends:**\n",
    "   - Sorting the scores helps visualize how different configurations affect the system's performance.\n",
    "   - Useful for pinpointing the impact of factors like chunk size, overlap, or embedding model.\n",
    "\n",
    "\n",
    "3. **Insights into Configurations:**\n",
    "   - Identifies which configurations yield higher-quality answers, guiding optimization efforts.\n",
    "   - Helps determine the best chunk size, overlap, or embedding model for the\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores = result.groupby(\"settings\")[\"eval_score_GPT4\"].mean()\n",
    "average_scores.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
